{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOQmSKA48P5cs+7NhOd+PI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danianamir/MultiAgentContiniousPCGRL/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dependencies"
      ],
      "metadata": {
        "id": "Ky19GpIKtKej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqAiJVcKsvTz"
      },
      "outputs": [],
      "source": [
        "#clone git\n",
        "!git clone https://github.com/danianamir/MultiAgentContiniousPCGRL.git\n",
        "%cd /content/MultiAgentContinousPCGRL\n",
        "!git pull origin main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#package installs with pip_github......................................................................\n",
        "%cd /content/MultiAgentContiniousPCGRL/ml-agents-envs\n",
        "!pip install .\n",
        "!pip install gymnasium\n",
        "!pip install ray[rllib]\n",
        "!pip install tf2onnx"
      ],
      "metadata": {
        "id": "WvJCUjecsxAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ml agents\n",
        "import mlagents_envs\n",
        "from mlagents_envs.environment import UnityEnvironment\n",
        "from mlagents_envs.base_env import ActionTuple\n",
        "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv\n",
        "\n",
        "\n",
        "#gymnasium\n",
        "import gymnasium as gym\n",
        "#gym space\n",
        "from gymnasium import spaces\n",
        "\n",
        "#pettingzoo\n",
        "import pettingzoo\n",
        "from pettingzoo import ParallelEnv\n",
        "from pettingzoo.utils import parallel_to_aec, wrappers ,agent_selector\n",
        "\n",
        "\n",
        "\n",
        "import functools\n",
        "\n",
        "\n",
        "\n",
        "#ray\n",
        "import ray\n",
        "from ray import air, tune\n",
        "from ray.rllib.examples.policy.random_policy import RandomPolicy\n",
        "from ray.rllib.evaluation.rollout_worker import RolloutWorker\n",
        "from ray.rllib.evaluation.worker_set import WorkerSet\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
        "from ray.rllib.algorithms.algorithm import Algorithm\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.policy.policy import PolicySpec\n",
        "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
        "\n",
        "from ray.rllib.algorithms.ppo.ppo import PPOConfig\n",
        "from ray.rllib.algorithms.ppo.ppo import PPO\n",
        "\n",
        "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
        "#others\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "from pprint import pprint\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.onnx as onnx\n",
        "\n",
        "#google\n",
        "from google.colab import files\n",
        "\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "KiovwkIxsy8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# give access to run the linux file ................................................................................\n",
        "%cd\n",
        "!chmod -R 755 /content/MultiAgentContiniousPCGRL/unity_built/my_game.x86_64\n",
        "!chmod -R 755 /content/MultiAgentContiniousPCGRL/unity_built/UnityPlayer.so\n",
        "!ls -l /content/MultiAgentContiniousPCGRL/unity_built"
      ],
      "metadata": {
        "id": "cZcIkb_Ps1Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unity environment"
      ],
      "metadata": {
        "id": "WFkwsVIMtP4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Unity3DEnv(MultiAgentEnv):\n",
        "\n",
        "    _BASE_PORT_EDITOR = 5004\n",
        "    _BASE_PORT_ENVIRONMENT = 5005\n",
        "    _WORKER_ID = 0\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        file_name: str = None,\n",
        "        port: Optional[int] = None,\n",
        "        seed: int = 0,\n",
        "        no_graphics: bool = False,\n",
        "        timeout_wait: int = 300,\n",
        "        episode_horizon: int = None,\n",
        "\n",
        "    ):\n",
        "\n",
        "        self._skip_env_checking = True\n",
        "        super().__init__()\n",
        "\n",
        "        if file_name is None:\n",
        "            print(\n",
        "                \"No game binary provided, will use a running Unity editor \"\n",
        "                \"instead.\\nMake sure you are pressing the Play (|>) button in \"\n",
        "                \"your editor to start.\"\n",
        "            )\n",
        "\n",
        "        # Try connecting to the Unity3D game instance. If a port is blocked\n",
        "        port_ = None\n",
        "        while True:\n",
        "            # Sleep for random time to allow for concurrent startup of many\n",
        "            # environments (num_workers >> 1). Otherwise, would lead to port\n",
        "            # conflicts sometimes.\n",
        "            if port_ is not None:\n",
        "                time.sleep(random.randint(1, 10))\n",
        "            port_ = port or (\n",
        "                self._BASE_PORT_ENVIRONMENT if file_name else self._BASE_PORT_EDITOR\n",
        "            )\n",
        "            # cache the worker_id and\n",
        "            # increase it for the next environment\n",
        "            worker_id_ = Unity3DEnv._WORKER_ID if file_name else 0\n",
        "            Unity3DEnv._WORKER_ID += 1\n",
        "            try:\n",
        "                self.unity_env = UnityEnvironment(\n",
        "                    file_name=file_name,\n",
        "                    worker_id=worker_id_,\n",
        "                    base_port=port_,\n",
        "                    seed=seed,\n",
        "                    no_graphics=no_graphics,\n",
        "                    timeout_wait=timeout_wait,\n",
        "                )\n",
        "\n",
        "                print(\"Created UnityEnvironment for port {}\".format(port_ + worker_id_))\n",
        "            except mlagents_envs.exception.UnityWorkerInUseException:\n",
        "                pass\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        self.episode_horizon = episode_horizon\n",
        "        self.episode_timesteps = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def step(\n",
        "        self, action_dict\n",
        "    ):\n",
        "\n",
        "        for behavior_name in self.unity_env.behavior_specs:\n",
        "                actions = []\n",
        "                for agent_id in self.unity_env.get_steps(behavior_name)[0].agent_id:\n",
        "                    key = behavior_name + \"_{}\".format(agent_id)\n",
        "                    actions.append(action_dict[key])\n",
        "\n",
        "\n",
        "\n",
        "                    if behavior_name==\"instantiator Behaviour?team=0\":\n",
        "                       continuous_action= np.array(actions[0][0])\n",
        "                       print(continuous_action)\n",
        "                       discrete_action=np.array([[actions[0][1]]])\n",
        "                       print(discrete_action)\n",
        "                       action_tuple =ActionTuple(continuous= continuous_action , discrete= discrete_action)\n",
        "\n",
        "                    if behavior_name==\"modifyer Behavior?team=0\":\n",
        "                       continuous_action = np.array(actions[0][0])\n",
        "                       print(continuous_action)\n",
        "                       discrete_action = np.array([actions[0][1]])\n",
        "                       print(discrete_action)\n",
        "                       action_tuple = ActionTuple(continuous= continuous_action , discrete= discrete_action)\n",
        "\n",
        "\n",
        "                    self.unity_env.set_actions(behavior_name, action_tuple)\n",
        "\n",
        "        self.unity_env.step()\n",
        "        self.episode_timesteps += 1\n",
        "        obs, rewards, terminateds, truncateds, infos = self._get_step_results()\n",
        "        return obs, rewards, terminateds, truncateds, infos\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def reset(\n",
        "        self, *, seed=None, options=None\n",
        "    ):\n",
        "        self.episode_timesteps = 0\n",
        "        self.unity_env.reset()\n",
        "        obs, _, _, _, infos = self._get_step_results()\n",
        "        return obs, infos\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _get_step_results(self):\n",
        "\n",
        "        # first we set the initial dict that step return\n",
        "        obs = {}\n",
        "        rewards = {}\n",
        "        terminated ={}\n",
        "        truncated={}\n",
        "        infos = {}\n",
        "\n",
        "\n",
        "        num_active=0\n",
        "        num_done=0\n",
        "        num_all=0\n",
        "\n",
        "\n",
        "\n",
        "        #go thorugh all the behavior\n",
        "        # return decision_step (batch of agent have similar behavior)\n",
        "        # return trminal steps  (batch of agent have similar behavior /that end thier episode)\n",
        "        for behavior_name  in self.unity_env.behavior_specs:\n",
        "            decision_steps, terminal_steps = self.unity_env.get_steps(behavior_name)\n",
        "            num_active=num_active+len(decision_steps)\n",
        "            num_done=num_done+len(terminal_steps)\n",
        "            num_all=num_active+num_done\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #set the obs / reward  from  decision step for each agent like:{\"behavior_name+agent_id\" : numpy array}\n",
        "            for agent_id, idx in decision_steps.agent_id_to_index.items():\n",
        "                key = behavior_name + \"_{}\".format(agent_id)\n",
        "                os = tuple(o[idx] for o in decision_steps.obs)\n",
        "                os = os[0] if len(os) == 1 else os\n",
        "                obs[key] = os\n",
        "                rewards[key] = (decision_steps.reward[idx] + decision_steps.group_reward[idx])\n",
        "\n",
        "\n",
        "\n",
        "            #set the obs / reward  from  terminal_steps for each agent like:{\"behavior_name+agent_id\" : numpy array}\n",
        "            for agent_id, idx in terminal_steps.agent_id_to_index.items():\n",
        "                key = behavior_name + \"_{}\".format(agent_id)\n",
        "                if key not in obs:\n",
        "                    os = tuple(o[idx] for o in terminal_steps.obs)\n",
        "                    obs[key] = os = os[0] if len(os) == 1 else os\n",
        "                rewards[key] = (terminal_steps.reward[idx] + terminal_steps.group_reward[idx])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #infos={\"acive\":num_active , \"done\":num_done , \"all\":num_all }\n",
        "\n",
        "\n",
        "        # set the terminated if all agent end thier own episod like: {\"__all__\":True}\n",
        "        if(num_active>0):\n",
        "           terminated[\"__all__\"]=False\n",
        "        else:\n",
        "           terminated[\"__all__\"]=True\n",
        "\n",
        "\n",
        "\n",
        "        # set the truncated /if the step of the env reach the horizen like{\"__all__\":True}\n",
        "        if self.episode_timesteps == self.episode_horizon:\n",
        "          truncated[\"__all__\"]=True\n",
        "        else:\n",
        "          truncated[\"__all__\"]=False\n",
        "\n",
        "\n",
        "        # return\n",
        "        return obs, rewards, terminated, truncated, infos\n"
      ],
      "metadata": {
        "id": "F82__vMfs3GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# make the rollout worker"
      ],
      "metadata": {
        "id": "cEJqOFOFtXb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_action_space_initializer = spaces.Box(low=-100, high=100, shape=(2,), dtype=float)\n",
        "discrete_action_space_initializer = spaces.Discrete(3)\n",
        "\n",
        "continuous_action_space_modifier=spaces.Box(low=-100, high=100, shape=(2,), dtype=float)\n",
        "multi_discrete_action_space_modifier=spaces.MultiDiscrete([6,2])\n",
        "\n",
        "\n",
        "\n",
        "policies = { \"policy_initializer\": (RandomPolicy,spaces.Box(float(\"-inf\"), float(\"inf\"), (28,)),\n",
        "                                    spaces.Tuple((continuous_action_space_initializer, discrete_action_space_initializer)),\n",
        "                                    AlgorithmConfig())\n",
        "\n",
        "                   ,\"policy_modifier\":(RandomPolicy,spaces.Box(float(\"-inf\"), float(\"inf\"), (28,)),\n",
        "                                       spaces.Tuple((continuous_action_space_modifier,multi_discrete_action_space_modifier)),\n",
        "                                       AlgorithmConfig())\n",
        "                }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
        "    if agent_id == 0:\n",
        "        return \"policy_initializer\"\n",
        "    else:\n",
        "        return \"policy_modifier\"\n",
        "\n",
        "\n",
        "config=AlgorithmConfig().multi_agent(policies=policies,policy_mapping_fn=policy_mapping_fn).rollouts(rollout_fragment_length=10)\n",
        "\n",
        "\n",
        "worker=RolloutWorker(env_creator=lambda _: Unity3DEnv(file_name=\"/content/MultiAgentContiniousPCGRL/unity_built/my_game.x86_64\",episode_horizon = 10)\n",
        ",config=config)\n"
      ],
      "metadata": {
        "id": "xo45oUxitEAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_batch=worker.sample()\n",
        "print(\"obs is {}\".format(multi_batch[\"policy_initializer\"][\"obs\"]))\n",
        "print(\"actions is{}\".format(multi_batch[\"policy_initializer\"][\"actions\"]))\n",
        "print(\"rewards is{}\".format(multi_batch[\"policy_initializer\"][\"rewards\"]))\n",
        "print(\"terminateds is{}\".format(multi_batch[\"policy_initializer\"][\"terminateds\"]))\n",
        "print(\"truncateds is{}\".format(multi_batch[\"policy_initializer\"][\"truncateds\"]))"
      ],
      "metadata": {
        "id": "yXAgSQlYtH9o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}